{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT for Like Prediction with Text Only Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_at</th>\n",
       "      <th>like_count_1h</th>\n",
       "      <th>like_count_2h</th>\n",
       "      <th>like_count_3h</th>\n",
       "      <th>like_count_4h</th>\n",
       "      <th>like_count_5h</th>\n",
       "      <th>like_count_6h</th>\n",
       "      <th>comment_count_1h</th>\n",
       "      <th>comment_count_2h</th>\n",
       "      <th>comment_count_3h</th>\n",
       "      <th>comment_count_4h</th>\n",
       "      <th>comment_count_5h</th>\n",
       "      <th>comment_count_6h</th>\n",
       "      <th>forum_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>forum_stats</th>\n",
       "      <th>like_count_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我的排骨湯</td>\n",
       "      <td>2022-10-05 14:20:21 UTC</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>598518</td>\n",
       "      <td>428921</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#請益 婚禮穿搭</td>\n",
       "      <td>2022-10-05 14:28:13 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>399302</td>\n",
       "      <td>650840</td>\n",
       "      <td>63.9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>無謂的啦啦隊</td>\n",
       "      <td>2022-10-06 07:18:22 UTC</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>650776</td>\n",
       "      <td>717288</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>文學理論 課本</td>\n",
       "      <td>2022-09-20 11:39:14 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>63</td>\n",
       "      <td>471023</td>\n",
       "      <td>173889</td>\n",
       "      <td>7.9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>一般課程</td>\n",
       "      <td>2022-09-05 10:18:24 UTC</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>230184</td>\n",
       "      <td>594332</td>\n",
       "      <td>36.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title               created_at  like_count_1h  like_count_2h  \\\n",
       "0     我的排骨湯  2022-10-05 14:20:21 UTC             12             15   \n",
       "1  #請益 婚禮穿搭  2022-10-05 14:28:13 UTC              0              0   \n",
       "2    無謂的啦啦隊  2022-10-06 07:18:22 UTC              3              7   \n",
       "3   文學理論 課本  2022-09-20 11:39:14 UTC              2              7   \n",
       "4      一般課程  2022-09-05 10:18:24 UTC              3              7   \n",
       "\n",
       "   like_count_3h  like_count_4h  like_count_5h  like_count_6h  \\\n",
       "0             15             15             16             18   \n",
       "1              3              4              4              4   \n",
       "2              8             11             12             14   \n",
       "3             11             24             26             26   \n",
       "4              7             10             10             11   \n",
       "\n",
       "   comment_count_1h  comment_count_2h  comment_count_3h  comment_count_4h  \\\n",
       "0                10                10                10                10   \n",
       "1                 2                 5                 8                 9   \n",
       "2                 1                 1                 2                 3   \n",
       "3                 2                 2                 8                32   \n",
       "4                15                26                35                38   \n",
       "\n",
       "   comment_count_5h  comment_count_6h  forum_id  author_id  forum_stats  \\\n",
       "0                10                10    598518     428921          0.7   \n",
       "1                 9                 9    399302     650840         63.9   \n",
       "2                 3                 3    650776     717288         19.2   \n",
       "3                38                63    471023     173889          7.9   \n",
       "4                48                49    230184     594332         36.2   \n",
       "\n",
       "   like_count_24h  \n",
       "0              26  \n",
       "1              11  \n",
       "2              19  \n",
       "3              29  \n",
       "4              16  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('../../raw_data/intern_homework_train_dataset.csv')\n",
    "test = pd.read_csv('../../raw_data/intern_homework_public_test_dataset.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, random_state=777, train_size=0.9)\n",
    "len(train), len(valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35094     15\n",
       "13209      8\n",
       "10950     15\n",
       "40900      6\n",
       "27226    266\n",
       "        ... \n",
       "26695    224\n",
       "36785     10\n",
       "40535     11\n",
       "15931     10\n",
       "47919     34\n",
       "Name: like_count_24h, Length: 45000, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = train['like_count_24h']\n",
    "valid_label = valid['like_count_24h']\n",
    "test_label = test['like_count_24h']\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = train_label.tolist()\n",
    "valid_label = valid_label.tolist()\n",
    "test_label = test_label.tolist()\n",
    "train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = list(map(float, train_label))\n",
    "valid_label = list(map(float, valid_label))\n",
    "test_label = list(map(float, test_label))\n",
    "train_label[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為 BART 為自然語言模型，label 也需要為文字的格式"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省略掉 作者 ID / 看板 ID / 看板資訊 因為我認為 BART 沒辦法了解這些 Feature, 會讓模型預測文字輸出帶來 noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "drop_columns = ['author_id', 'like_count_24h', 'forum_id', 'forum_stats']\n",
    "\n",
    "train_input = train.drop(drop_columns, axis=1)\n",
    "valid_input = valid.drop(drop_columns, axis=1)\n",
    "test_input = test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfromations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理 created_by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>like_count_1h</th>\n",
       "      <th>like_count_2h</th>\n",
       "      <th>like_count_3h</th>\n",
       "      <th>like_count_4h</th>\n",
       "      <th>like_count_5h</th>\n",
       "      <th>like_count_6h</th>\n",
       "      <th>comment_count_1h</th>\n",
       "      <th>comment_count_2h</th>\n",
       "      <th>comment_count_3h</th>\n",
       "      <th>comment_count_4h</th>\n",
       "      <th>comment_count_5h</th>\n",
       "      <th>comment_count_6h</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35094</th>\n",
       "      <td>111台聯A3台綜A10轉學考上榜心得</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13209</th>\n",
       "      <td>［世界志工社｜第二次招生說明會預告］</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>「攜手」-我們在身份轉換中的震盪</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40900</th>\n",
       "      <td>怎麼抓住小一女生的心</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27226</th>\n",
       "      <td>楊丞琳以前家境清寒吃不起海鮮</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  like_count_1h  like_count_2h  like_count_3h  \\\n",
       "35094  111台聯A3台綜A10轉學考上榜心得              1              5              7   \n",
       "13209   ［世界志工社｜第二次招生說明會預告］              7              7              7   \n",
       "10950     「攜手」-我們在身份轉換中的震盪              3              3              3   \n",
       "40900           怎麼抓住小一女生的心              0              0              0   \n",
       "27226       楊丞琳以前家境清寒吃不起海鮮              2              3              5   \n",
       "\n",
       "       like_count_4h  like_count_5h  like_count_6h  comment_count_1h  \\\n",
       "35094              8              9             10                 2   \n",
       "13209              7              7              7                21   \n",
       "10950              3              3              4                 5   \n",
       "40900              0              0              0                 1   \n",
       "27226              5              5              5                 0   \n",
       "\n",
       "       comment_count_2h  comment_count_3h  comment_count_4h  comment_count_5h  \\\n",
       "35094                 2                 6                 7                 7   \n",
       "13209                22                22                22                22   \n",
       "10950                 5                 5                 5                 5   \n",
       "40900                 4                 4                 4                 4   \n",
       "27226                 0                 1                 1                 1   \n",
       "\n",
       "       comment_count_6h  weekday  hour  \n",
       "35094                 7        5    12  \n",
       "13209                22        6    13  \n",
       "10950                 5        5    12  \n",
       "40900                 5        1    12  \n",
       "27226                 1        1    16  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = split_date(train_input, 'created_at')\n",
    "valid_input = split_date(valid_input, 'created_at')\n",
    "test_input = split_date(test_input, 'created_at')\n",
    "\n",
    "# 顯示處理過的資料集\n",
    "train_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'like_count_1h', 'like_count_2h', 'like_count_3h',\n",
       "       'like_count_4h', 'like_count_5h', 'like_count_6h', 'comment_count_1h',\n",
       "       'comment_count_2h', 'comment_count_3h', 'comment_count_4h',\n",
       "       'comment_count_5h', 'comment_count_6h', 'weekday', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自訂義 dict，將 weekday 轉換為中文星期幾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_dict = {\n",
    "    0: '星期一',\n",
    "    1: '星期二',\n",
    "    2: '星期三',\n",
    "    3: '星期四',\n",
    "    4: '星期五',\n",
    "    5: '星期六',\n",
    "    6: '星期日'\n",
    "}\n",
    "\n",
    "# 將 weekday 轉換為中文星期幾\n",
    "train_input['weekday'] = train_input['weekday'].map(weekday_dict)\n",
    "valid_input['weekday'] = valid_input['weekday'].map(weekday_dict)\n",
    "test_input['weekday'] = test_input['weekday'].map(weekday_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 Input 的數值與文字合併一篇文章來當作 BERT Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dataframe 內的 feature 合併成一串文字 的函數\n",
    "def transfrom_to_text(df):\n",
    "\n",
    "    passage = \"\"\n",
    "\n",
    "    # combined title and post time\n",
    "    passage += \"這篇文章標題是 {} ，在{}{}點發佈。\".format(df[\"title\"], df[\"weekday\"], df[\"hour\"])\n",
    "\n",
    "    # combined likes_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的愛心數有 {}，在第二小時累積到的愛心數有 {}，\".format(df[\"like_count_1h\"], df[\"like_count_2h\"])\n",
    "    passage += \"第三小時累積到的愛心數有 {}，在第四小時累積到的愛心數有 {}，\".format(df[\"like_count_3h\"], df[\"like_count_4h\"])\n",
    "    passage += \"第五小時累積到的愛心數有 {}，在第六小時累積到的愛心數有 {}。\".format(df[\"like_count_5h\"], df[\"like_count_6h\"])\n",
    "\n",
    "    # combined comment_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的留言數有 {}，在第二小時累積到的留言數有 {}，\".format(df[\"comment_count_1h\"], df[\"comment_count_2h\"])\n",
    "    passage += \"第三小時累積到的留言數有 {}，在第四小時累積到的留言數有 {}，\".format(df[\"comment_count_3h\"], df[\"comment_count_4h\"])\n",
    "    passage += \"第五小時累積到的留言數有 {}，在第六小時累積到的留言數有 {}。\".format(df[\"comment_count_5h\"], df[\"comment_count_6h\"])\n",
    "\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'這篇文章標題是 111台聯A3台綜A10轉學考上榜心得 ，在星期六12點發佈。文章在發佈後的第一小時累積到的愛心數有 1，在第二小時累積到的愛心數有 5，第三小時累積到的愛心數有 7，在第四小時累積到的愛心數有 8，第五小時累積到的愛心數有 9，在第六小時累積到的愛心數有 10。文章在發佈後的第一小時累積到的留言數有 2，在第二小時累積到的留言數有 2，第三小時累積到的留言數有 6，在第四小時累積到的留言數有 7，第五小時累積到的留言數有 7，在第六小時累積到的留言數有 7。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = train_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "valid_text = valid_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "test_text = test_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 總共有 45000 筆\n",
      "Valid Data 總共有 5000 筆\n",
      "Test Data 總共有 10000 筆\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data 總共有 {} 筆\".format(len(train_text)))\n",
    "print(\"Valid Data 總共有 {} 筆\".format(len(valid_text)))\n",
    "print(\"Test Data 總共有 {} 筆\".format(len(test_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 19:31:25.961254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 19:31:26.069395: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-06 19:31:26.553874: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-06 19:31:26.554033: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-06 19:31:26.554040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_text ,truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_text ,truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text ,truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(encodings, label):\n",
    "  encodings.update({'labels': label})\n",
    "\n",
    "add_label(train_encodings, train_label)\n",
    "add_label(valid_encodings, valid_label)\n",
    "add_label(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, default_data_collator\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = Dataset(train_encodings)\n",
    "valid_dataset = Dataset(valid_encodings)\n",
    "test_dataset = Dataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"MAPE\",\n",
    "    weight_decay=0.01,\n",
    "    eval_accumulation_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "   \n",
    "    # evaluation metrics\n",
    "    MAPE = mean_absolute_percentage_error(labels, predictions)\n",
    "    \n",
    "    result = {'MAPE': MAPE}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 45000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4221\n",
      "  Number of trainable parameters = 102268417\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhankystyle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/Dcard/model/text_only/wandb/run-20230406_193238-gu75m73c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hankystyle/huggingface/runs/gu75m73c' target=\"_blank\">emissary-seven-47</a></strong> to <a href='https://wandb.ai/hankystyle/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hankystyle/huggingface' target=\"_blank\">https://wandb.ai/hankystyle/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hankystyle/huggingface/runs/gu75m73c' target=\"_blank\">https://wandb.ai/hankystyle/huggingface/runs/gu75m73c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='4221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  64/4221 00:27 < 31:11, 2.22 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1407\n",
      "Configuration saved in ./results/checkpoint-1407/config.json\n",
      "Model weights saved in ./results/checkpoint-1407/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1407/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1407/special_tokens_map.json\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2814\n",
      "Configuration saved in ./results/checkpoint-2814/config.json\n",
      "Model weights saved in ./results/checkpoint-2814/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2814/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2814/special_tokens_map.json\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-4221\n",
      "Configuration saved in ./results/checkpoint-4221/config.json\n",
      "Model weights saved in ./results/checkpoint-4221/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4221/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4221/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1407] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-2814 (score: 0.6166888475418091).\n",
      "Deleting older checkpoint [results/checkpoint-2814] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-4221] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4221, training_loss=32057.68372423596, metrics={'train_runtime': 2374.3914, 'train_samples_per_second': 56.857, 'train_steps_per_second': 1.778, 'total_flos': 2.98309758363e+16, 'train_loss': 32057.68372423596, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 38338.796875,\n",
       " 'eval_MAPE': 0.6166888475418091,\n",
       " 'eval_runtime': 19.6004,\n",
       " 'eval_samples_per_second': 255.096,\n",
       " 'eval_steps_per_second': 8.01,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_model/bert-base-dcard\n",
      "Configuration saved in saved_model/bert-base-dcard/config.json\n",
      "Model weights saved in saved_model/bert-base-dcard/pytorch_model.bin\n",
      "tokenizer config file saved in saved_model/bert-base-dcard/tokenizer_config.json\n",
      "Special tokens file saved in saved_model/bert-base-dcard/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('saved_model/bert-base-dcard')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "private_test = pd.read_csv('../../raw_data/intern_homework_private_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "drop_columns = ['author_id', 'forum_id', 'forum_stats']\n",
    "private_test_input = private_test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_input = split_date(private_test_input, 'created_at')\n",
    "\n",
    "weekday_dict = {\n",
    "    0: '星期一',\n",
    "    1: '星期二',\n",
    "    2: '星期三',\n",
    "    3: '星期四',\n",
    "    4: '星期五',\n",
    "    5: '星期六',\n",
    "    6: '星期日'\n",
    "}\n",
    "\n",
    "# 將 weekday 轉換為中文星期幾\n",
    "private_test_input['weekday'] = private_test_input['weekday'].map(weekday_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dataframe 內的 feature 合併成一串文字 的函數\n",
    "def transfrom_to_text(df):\n",
    "\n",
    "    passage = \"\"\n",
    "\n",
    "    # combined title and post time\n",
    "    passage += \"這篇文章標題是 {} ，在{}{}點發佈。\".format(df[\"title\"], df[\"weekday\"], df[\"hour\"])\n",
    "\n",
    "    # combined likes_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的愛心數有 {}，在第二小時累積到的愛心數有 {}，\".format(df[\"like_count_1h\"], df[\"like_count_2h\"])\n",
    "    passage += \"第三小時累積到的愛心數有 {}，在第四小時累積到的愛心數有 {}，\".format(df[\"like_count_3h\"], df[\"like_count_4h\"])\n",
    "    passage += \"第五小時累積到的愛心數有 {}，在第六小時累積到的愛心數有 {}。\".format(df[\"like_count_5h\"], df[\"like_count_6h\"])\n",
    "\n",
    "    # combined comment_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的留言數有 {}，在第二小時累積到的留言數有 {}，\".format(df[\"comment_count_1h\"], df[\"comment_count_2h\"])\n",
    "    passage += \"第三小時累積到的留言數有 {}，在第四小時累積到的留言數有 {}，\".format(df[\"comment_count_3h\"], df[\"comment_count_4h\"])\n",
    "    passage += \"第五小時累積到的留言數有 {}，在第六小時累積到的留言數有 {}。\".format(df[\"comment_count_5h\"], df[\"comment_count_6h\"])\n",
    "\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'這篇文章標題是 #心情 頂樓風好大 ，在星期四0點發佈。文章在發佈後的第一小時累積到的愛心數有 6，在第二小時累積到的愛心數有 9，第三小時累積到的愛心數有 10，在第四小時累積到的愛心數有 12，第五小時累積到的愛心數有 12，在第六小時累積到的愛心數有 16。文章在發佈後的第一小時累積到的留言數有 4，在第二小時累積到的留言數有 8，第三小時累積到的留言數有 9，在第四小時累積到的留言數有 11，第五小時累積到的留言數有 12，在第六小時累積到的留言數有 12。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_test_text = private_test_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "private_test_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測資料總共有 10000 筆\n"
     ]
    }
   ],
   "source": [
    "print(\"預測資料總共有 {} 筆\".format(len(private_test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 20:26:01.756913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 20:26:01.862095: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-06 20:26:02.303974: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-06 20:26:02.304095: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-06 20:26:02.304103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, default_data_collator\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"saved_model/bert-base-dcard\", num_labels=1)\n",
    "private_test_encodings = tokenizer(private_test_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "private_test_dataset = Dataset(private_test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # evaluation metrics\n",
    "    MAPE = mean_absolute_percentage_error(labels, predictions)\n",
    "    \n",
    "    result = {'MAPE': MAPE}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [624/625 00:40 < 00:00, 15.42 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(private_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_runtime': 42.2636,\n",
       " 'test_samples_per_second': 236.61,\n",
       " 'test_steps_per_second': 14.788}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(map(int,predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=[\"like_count_24h\"])\n",
    "df.to_csv(\"result(bert).csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
