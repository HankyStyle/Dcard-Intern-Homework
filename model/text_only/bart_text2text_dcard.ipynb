{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BART for Like Prediction with Text Only Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_at</th>\n",
       "      <th>like_count_1h</th>\n",
       "      <th>like_count_2h</th>\n",
       "      <th>like_count_3h</th>\n",
       "      <th>like_count_4h</th>\n",
       "      <th>like_count_5h</th>\n",
       "      <th>like_count_6h</th>\n",
       "      <th>comment_count_1h</th>\n",
       "      <th>comment_count_2h</th>\n",
       "      <th>comment_count_3h</th>\n",
       "      <th>comment_count_4h</th>\n",
       "      <th>comment_count_5h</th>\n",
       "      <th>comment_count_6h</th>\n",
       "      <th>forum_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>forum_stats</th>\n",
       "      <th>like_count_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我的排骨湯</td>\n",
       "      <td>2022-10-05 14:20:21 UTC</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>598518</td>\n",
       "      <td>428921</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#請益 婚禮穿搭</td>\n",
       "      <td>2022-10-05 14:28:13 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>399302</td>\n",
       "      <td>650840</td>\n",
       "      <td>63.9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>無謂的啦啦隊</td>\n",
       "      <td>2022-10-06 07:18:22 UTC</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>650776</td>\n",
       "      <td>717288</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>文學理論 課本</td>\n",
       "      <td>2022-09-20 11:39:14 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>63</td>\n",
       "      <td>471023</td>\n",
       "      <td>173889</td>\n",
       "      <td>7.9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>一般課程</td>\n",
       "      <td>2022-09-05 10:18:24 UTC</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>230184</td>\n",
       "      <td>594332</td>\n",
       "      <td>36.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title               created_at  like_count_1h  like_count_2h  \\\n",
       "0     我的排骨湯  2022-10-05 14:20:21 UTC             12             15   \n",
       "1  #請益 婚禮穿搭  2022-10-05 14:28:13 UTC              0              0   \n",
       "2    無謂的啦啦隊  2022-10-06 07:18:22 UTC              3              7   \n",
       "3   文學理論 課本  2022-09-20 11:39:14 UTC              2              7   \n",
       "4      一般課程  2022-09-05 10:18:24 UTC              3              7   \n",
       "\n",
       "   like_count_3h  like_count_4h  like_count_5h  like_count_6h  \\\n",
       "0             15             15             16             18   \n",
       "1              3              4              4              4   \n",
       "2              8             11             12             14   \n",
       "3             11             24             26             26   \n",
       "4              7             10             10             11   \n",
       "\n",
       "   comment_count_1h  comment_count_2h  comment_count_3h  comment_count_4h  \\\n",
       "0                10                10                10                10   \n",
       "1                 2                 5                 8                 9   \n",
       "2                 1                 1                 2                 3   \n",
       "3                 2                 2                 8                32   \n",
       "4                15                26                35                38   \n",
       "\n",
       "   comment_count_5h  comment_count_6h  forum_id  author_id  forum_stats  \\\n",
       "0                10                10    598518     428921          0.7   \n",
       "1                 9                 9    399302     650840         63.9   \n",
       "2                 3                 3    650776     717288         19.2   \n",
       "3                38                63    471023     173889          7.9   \n",
       "4                48                49    230184     594332         36.2   \n",
       "\n",
       "   like_count_24h  \n",
       "0              26  \n",
       "1              11  \n",
       "2              19  \n",
       "3              29  \n",
       "4              16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('../../raw_data/intern_homework_train_dataset.csv')\n",
    "test = pd.read_csv('../../raw_data/intern_homework_public_test_dataset.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, random_state=777, train_size=0.9)\n",
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['like_count_24h']\n",
    "valid_label = valid['like_count_24h']\n",
    "test_label = test['like_count_24h']\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.tolist()\n",
    "valid_label = valid_label.tolist()\n",
    "test_label = test_label.tolist()\n",
    "train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = list(map(str, train_label))\n",
    "valid_label = list(map(str, valid_label))\n",
    "test_label = list(map(str, test_label))\n",
    "train_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為 BART 為自然語言模型，label 也需要為文字的格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省略掉 作者 ID / 看板 ID / 看板資訊 因為我認為 BART 沒辦法了解這些 Feature, 會讓模型預測文字輸出帶來 noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "drop_columns = ['author_id', 'like_count_24h', 'forum_id', 'forum_stats']\n",
    "\n",
    "train_input = train.drop(drop_columns, axis=1)\n",
    "valid_input = valid.drop(drop_columns, axis=1)\n",
    "test_input = test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfromations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理 created_by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = split_date(train_input, 'created_at')\n",
    "valid_input = split_date(valid_input, 'created_at')\n",
    "test_input = split_date(test_input, 'created_at')\n",
    "\n",
    "# 顯示處理過的資料集\n",
    "train_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自訂義 dict，將 weekday 轉換為中文星期幾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_dict = {\n",
    "    0: '星期一',\n",
    "    1: '星期二',\n",
    "    2: '星期三',\n",
    "    3: '星期四',\n",
    "    4: '星期五',\n",
    "    5: '星期六',\n",
    "    6: '星期日'\n",
    "}\n",
    "\n",
    "# 將 weekday 轉換為中文星期幾\n",
    "train_input['weekday'] = train_input['weekday'].map(weekday_dict)\n",
    "valid_input['weekday'] = valid_input['weekday'].map(weekday_dict)\n",
    "test_input['weekday'] = test_input['weekday'].map(weekday_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 Input 的數值與文字合併一篇文章來當作 BART Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dataframe 內的 feature 合併成一串文字 的函數\n",
    "def transfrom_to_text(df):\n",
    "\n",
    "    passage = \"\"\n",
    "\n",
    "    # combined title and post time\n",
    "    passage += \"這篇文章標題是 {} ，在{}{}點發佈。\".format(df[\"title\"], df[\"weekday\"], df[\"hour\"])\n",
    "\n",
    "    # combined likes_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的愛心數有 {}，在第二小時累積到的愛心數有 {}，\".format(df[\"like_count_1h\"], df[\"like_count_2h\"])\n",
    "    passage += \"第三小時累積到的愛心數有 {}，在第四小時累積到的愛心數有 {}，\".format(df[\"like_count_3h\"], df[\"like_count_4h\"])\n",
    "    passage += \"第五小時累積到的愛心數有 {}，在第六小時累積到的愛心數有 {}。\".format(df[\"like_count_5h\"], df[\"like_count_6h\"])\n",
    "\n",
    "    # combined comment_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的留言數有 {}，在第二小時累積到的留言數有 {}，\".format(df[\"comment_count_1h\"], df[\"comment_count_2h\"])\n",
    "    passage += \"第三小時累積到的留言數有 {}，在第四小時累積到的留言數有 {}，\".format(df[\"comment_count_3h\"], df[\"comment_count_4h\"])\n",
    "    passage += \"第五小時累積到的留言數有 {}，在第六小時累積到的留言數有 {}。\".format(df[\"comment_count_5h\"], df[\"comment_count_6h\"])\n",
    "\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "valid_text = valid_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "test_text = test_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data 總共有 {} 筆\".format(len(train_text)))\n",
    "print(\"Valid Data 總共有 {} 筆\".format(len(valid_text)))\n",
    "print(\"Test Data 總共有 {} 筆\".format(len(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 07:00:20.767929: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 07:00:20.871585: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-08 07:00:21.333055: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-08 07:00:21.333168: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-08 07:00:21.333175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_text ,text_target = train_label, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_text ,text_target = valid_label, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text ,text_target = test_label, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"fnlp/bart-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = Dataset(train_encodings)\n",
    "valid_dataset = Dataset(valid_encodings)\n",
    "test_dataset = Dataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"MAPE\",\n",
    "    greater_is_better = False,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all sentences\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    for k in range(len(decoded_labels)):\n",
    "\n",
    "        match = re.search(r'\\d+', decoded_preds[k])\n",
    "        if match:\n",
    "            pred = int(match.group())\n",
    "        label = int(decoded_labels[k])\n",
    "\n",
    "        predicted.append(pred)\n",
    "        true_label.append(label)\n",
    "\n",
    "    # evaluation metrics\n",
    "    MAPE = mean_absolute_percentage_error(true_label, predicted)\n",
    "    \n",
    "    result = {'MAPE': MAPE}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('saved_model/bart-base-text2text-dcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('在 test dataset 的表現\\n',metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "private_test = pd.read_csv('../../raw_data/intern_homework_private_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "drop_columns = ['author_id', 'forum_id', 'forum_stats']\n",
    "private_test_input = private_test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_input = split_date(private_test_input, 'created_at')\n",
    "\n",
    "weekday_dict = {\n",
    "    0: '星期一',\n",
    "    1: '星期二',\n",
    "    2: '星期三',\n",
    "    3: '星期四',\n",
    "    4: '星期五',\n",
    "    5: '星期六',\n",
    "    6: '星期日'\n",
    "}\n",
    "\n",
    "# 將 weekday 轉換為中文星期幾\n",
    "private_test_input['weekday'] = private_test_input['weekday'].map(weekday_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dataframe 內的 feature 合併成一串文字 的函數\n",
    "def transfrom_to_text(df):\n",
    "\n",
    "    passage = \"\"\n",
    "\n",
    "    # combined title and post time\n",
    "    passage += \"這篇文章標題是 {} ，在{}{}點發佈。\".format(df[\"title\"], df[\"weekday\"], df[\"hour\"])\n",
    "\n",
    "    # combined likes_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的愛心數有 {}，在第二小時累積到的愛心數有 {}，\".format(df[\"like_count_1h\"], df[\"like_count_2h\"])\n",
    "    passage += \"第三小時累積到的愛心數有 {}，在第四小時累積到的愛心數有 {}，\".format(df[\"like_count_3h\"], df[\"like_count_4h\"])\n",
    "    passage += \"第五小時累積到的愛心數有 {}，在第六小時累積到的愛心數有 {}。\".format(df[\"like_count_5h\"], df[\"like_count_6h\"])\n",
    "\n",
    "    # combined comment_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的留言數有 {}，在第二小時累積到的留言數有 {}，\".format(df[\"comment_count_1h\"], df[\"comment_count_2h\"])\n",
    "    passage += \"第三小時累積到的留言數有 {}，在第四小時累積到的留言數有 {}，\".format(df[\"comment_count_3h\"], df[\"comment_count_4h\"])\n",
    "    passage += \"第五小時累積到的留言數有 {}，在第六小時累積到的留言數有 {}。\".format(df[\"comment_count_5h\"], df[\"comment_count_6h\"])\n",
    "\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_text = private_test_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "private_test_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"預測資料總共有 {} 筆\".format(len(private_test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "private_test_encodings = tokenizer(private_test_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "private_test_dataset = Dataset(private_test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(private_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "decoded_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(decoded_preds, columns=[\"like_count_24h\"])\n",
    "df.to_csv(\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
