{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BART for Like Prediction with Text Only Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('../../raw_data/intern_homework_train_dataset.csv')\n",
    "test = pd.read_csv('../../raw_data/intern_homework_public_test_dataset.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, random_state=777, train_size=0.9)\n",
    "len(train), len(valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['like_count_24h']\n",
    "valid_label = valid['like_count_24h']\n",
    "test_label = test['like_count_24h']\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.tolist()\n",
    "valid_label = valid_label.tolist()\n",
    "test_label = test_label.tolist()\n",
    "train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = list(map(str, train_label))\n",
    "valid_label = list(map(str, valid_label))\n",
    "test_label = list(map(str, test_label))\n",
    "train_label[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為 BART 為自然語言模型，label 也需要為文字的格式"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省略掉 作者 ID  因為我認為 BART 沒辦法了解這些 Feature, 會讓模型預測文字輸出帶來 noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "drop_columns = ['author_id', 'like_count_24h']\n",
    "\n",
    "train_input = train.drop(drop_columns, axis=1)\n",
    "valid_input = valid.drop(drop_columns, axis=1)\n",
    "test_input = test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfromations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理 created_by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = split_date(train_input, 'created_at')\n",
    "valid_input = split_date(valid_input, 'created_at')\n",
    "test_input = split_date(test_input, 'created_at')\n",
    "\n",
    "# 顯示處理過的資料集\n",
    "train_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自訂義 dict，將 weekday 轉換為中文星期幾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_dict = {\n",
    "    0: '星期一',\n",
    "    1: '星期二',\n",
    "    2: '星期三',\n",
    "    3: '星期四',\n",
    "    4: '星期五',\n",
    "    5: '星期六',\n",
    "    6: '星期日'\n",
    "}\n",
    "\n",
    "# 將 weekday 轉換為中文星期幾\n",
    "train_input['weekday'] = train_input['weekday'].map(weekday_dict)\n",
    "valid_input['weekday'] = valid_input['weekday'].map(weekday_dict)\n",
    "test_input['weekday'] = test_input['weekday'].map(weekday_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 Input 的數值與文字合併一篇文章來當作 BART Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dataframe 內的 feature 合併成一串文字 的函數\n",
    "def transfrom_to_text(df):\n",
    "\n",
    "    passage = \"\"\n",
    "\n",
    "    # combined title and post time\n",
    "    passage += \"這篇文章標題是 {} ，在{}{}點發佈。\".format(df[\"title\"], df[\"weekday\"], df[\"hour\"])\n",
    "\n",
    "    # combined forum_id and forum_stats\n",
    "    passage += \"這篇發佈的看板 ID 為 {}， 看板資訊為 {} 。\".format(df[\"forum_id\"], df[\"forum_stats\"])\n",
    "\n",
    "    # combined likes_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的愛心數有 {}，在第二小時累積到的愛心數有 {}，\".format(df[\"like_count_1h\"], df[\"like_count_2h\"])\n",
    "    passage += \"第三小時累積到的愛心數有 {}，在第四小時累積到的愛心數有 {}，\".format(df[\"like_count_3h\"], df[\"like_count_4h\"])\n",
    "    passage += \"第五小時累積到的愛心數有 {}，在第六小時累積到的愛心數有 {}。\".format(df[\"like_count_5h\"], df[\"like_count_6h\"])\n",
    "\n",
    "    # combined comment_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的留言數有 {}，在第二小時累積到的留言數有 {}，\".format(df[\"comment_count_1h\"], df[\"comment_count_2h\"])\n",
    "    passage += \"第三小時累積到的留言數有 {}，在第四小時累積到的留言數有 {}，\".format(df[\"comment_count_3h\"], df[\"comment_count_4h\"])\n",
    "    passage += \"第五小時累積到的留言數有 {}，在第六小時累積到的留言數有 {}。\".format(df[\"comment_count_5h\"], df[\"comment_count_6h\"])\n",
    "\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "valid_text = valid_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "test_text = test_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data 總共有 {} 筆\".format(len(train_text)))\n",
    "print(\"Valid Data 總共有 {} 筆\".format(len(valid_text)))\n",
    "print(\"Test Data 總共有 {} 筆\".format(len(test_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_text ,text_target = train_label, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_text ,text_target = valid_label, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text ,text_target = test_label, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"fnlp/bart-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = Dataset(train_encodings)\n",
    "valid_dataset = Dataset(valid_encodings)\n",
    "test_dataset = Dataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"MAPE\",\n",
    "    greater_is_better = False,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all sentences\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    for k in range(len(decoded_labels)):\n",
    "\n",
    "        match = re.search(r'\\d+', decoded_preds[k])\n",
    "        if match:\n",
    "            pred = int(match.group())\n",
    "        label = int(decoded_labels[k])\n",
    "\n",
    "        predicted.append(pred)\n",
    "        true_label.append(label)\n",
    "\n",
    "    # evaluation metrics\n",
    "    MAPE = mean_absolute_percentage_error(true_label, predicted)\n",
    "    \n",
    "    result = {'MAPE': MAPE}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('saved_model/bart-base-text2text-dcard-all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "private_test = pd.read_csv('../../raw_data/intern_homework_private_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "drop_columns = ['author_id']\n",
    "private_test_input = private_test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_input = split_date(private_test_input, 'created_at')\n",
    "\n",
    "weekday_dict = {\n",
    "    0: '星期一',\n",
    "    1: '星期二',\n",
    "    2: '星期三',\n",
    "    3: '星期四',\n",
    "    4: '星期五',\n",
    "    5: '星期六',\n",
    "    6: '星期日'\n",
    "}\n",
    "\n",
    "# 將 weekday 轉換為中文星期幾\n",
    "private_test_input['weekday'] = private_test_input['weekday'].map(weekday_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dataframe 內的 feature 合併成一串文字 的函數\n",
    "def transfrom_to_text(df):\n",
    "\n",
    "    passage = \"\"\n",
    "\n",
    "    # combined title and post time\n",
    "    passage += \"這篇文章標題是 {} ，在{}{}點發佈。\".format(df[\"title\"], df[\"weekday\"], df[\"hour\"])\n",
    "\n",
    "    # combined forum_id and forum_stats\n",
    "    passage += \"這篇發佈的看板 ID 為 {}， 看板資訊為 {} 。\".format(df[\"forum_id\"], df[\"forum_stats\"])\n",
    "\n",
    "    # combined likes_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的愛心數有 {}，在第二小時累積到的愛心數有 {}，\".format(df[\"like_count_1h\"], df[\"like_count_2h\"])\n",
    "    passage += \"第三小時累積到的愛心數有 {}，在第四小時累積到的愛心數有 {}，\".format(df[\"like_count_3h\"], df[\"like_count_4h\"])\n",
    "    passage += \"第五小時累積到的愛心數有 {}，在第六小時累積到的愛心數有 {}。\".format(df[\"like_count_5h\"], df[\"like_count_6h\"])\n",
    "\n",
    "    # combined comment_count\n",
    "    passage += \"文章在發佈後的第一小時累積到的留言數有 {}，在第二小時累積到的留言數有 {}，\".format(df[\"comment_count_1h\"], df[\"comment_count_2h\"])\n",
    "    passage += \"第三小時累積到的留言數有 {}，在第四小時累積到的留言數有 {}，\".format(df[\"comment_count_3h\"], df[\"comment_count_4h\"])\n",
    "    passage += \"第五小時累積到的留言數有 {}，在第六小時累積到的留言數有 {}。\".format(df[\"comment_count_5h\"], df[\"comment_count_6h\"])\n",
    "\n",
    "    \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_text = private_test_input.apply(transfrom_to_text, axis=1).tolist()\n",
    "private_test_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"預測資料總共有 {} 筆\".format(len(private_test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-base-chinese\")\n",
    "private_test_encodings = tokenizer(private_test_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "private_test_dataset = Dataset(private_test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(private_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "decoded_preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_int(label):\n",
    "    match = re.search(r'\\d+', label)\n",
    "    int_label = int(match.group())\n",
    "    return int_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(map(get_int,decoded_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds, columns=[\"like_count_24h\"])\n",
    "df.to_csv(\"result(without_author_id).csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
