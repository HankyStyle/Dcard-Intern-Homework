{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing BERT with Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定 建議使用 Colab 跑訓練與預測 並且 執行階段使用 GPU\n",
    "基本 import 安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install numpy\n",
    "# !pip install -U matplotlib\n",
    "# !pip install transformer\n",
    "# !pip install multimodal-transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_at</th>\n",
       "      <th>like_count_1h</th>\n",
       "      <th>like_count_2h</th>\n",
       "      <th>like_count_3h</th>\n",
       "      <th>like_count_4h</th>\n",
       "      <th>like_count_5h</th>\n",
       "      <th>like_count_6h</th>\n",
       "      <th>comment_count_1h</th>\n",
       "      <th>comment_count_2h</th>\n",
       "      <th>comment_count_3h</th>\n",
       "      <th>comment_count_4h</th>\n",
       "      <th>comment_count_5h</th>\n",
       "      <th>comment_count_6h</th>\n",
       "      <th>forum_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>forum_stats</th>\n",
       "      <th>like_count_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我的排骨湯</td>\n",
       "      <td>2022-10-05 14:20:21 UTC</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>598518</td>\n",
       "      <td>428921</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#請益 婚禮穿搭</td>\n",
       "      <td>2022-10-05 14:28:13 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>399302</td>\n",
       "      <td>650840</td>\n",
       "      <td>63.9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>無謂的啦啦隊</td>\n",
       "      <td>2022-10-06 07:18:22 UTC</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>650776</td>\n",
       "      <td>717288</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>文學理論 課本</td>\n",
       "      <td>2022-09-20 11:39:14 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>63</td>\n",
       "      <td>471023</td>\n",
       "      <td>173889</td>\n",
       "      <td>7.9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>一般課程</td>\n",
       "      <td>2022-09-05 10:18:24 UTC</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>230184</td>\n",
       "      <td>594332</td>\n",
       "      <td>36.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title               created_at  like_count_1h  like_count_2h  \\\n",
       "0     我的排骨湯  2022-10-05 14:20:21 UTC             12             15   \n",
       "1  #請益 婚禮穿搭  2022-10-05 14:28:13 UTC              0              0   \n",
       "2    無謂的啦啦隊  2022-10-06 07:18:22 UTC              3              7   \n",
       "3   文學理論 課本  2022-09-20 11:39:14 UTC              2              7   \n",
       "4      一般課程  2022-09-05 10:18:24 UTC              3              7   \n",
       "\n",
       "   like_count_3h  like_count_4h  like_count_5h  like_count_6h  \\\n",
       "0             15             15             16             18   \n",
       "1              3              4              4              4   \n",
       "2              8             11             12             14   \n",
       "3             11             24             26             26   \n",
       "4              7             10             10             11   \n",
       "\n",
       "   comment_count_1h  comment_count_2h  comment_count_3h  comment_count_4h  \\\n",
       "0                10                10                10                10   \n",
       "1                 2                 5                 8                 9   \n",
       "2                 1                 1                 2                 3   \n",
       "3                 2                 2                 8                32   \n",
       "4                15                26                35                38   \n",
       "\n",
       "   comment_count_5h  comment_count_6h  forum_id  author_id  forum_stats  \\\n",
       "0                10                10    598518     428921          0.7   \n",
       "1                 9                 9    399302     650840         63.9   \n",
       "2                 3                 3    650776     717288         19.2   \n",
       "3                38                63    471023     173889          7.9   \n",
       "4                48                49    230184     594332         36.2   \n",
       "\n",
       "   like_count_24h  \n",
       "0              26  \n",
       "1              11  \n",
       "2              19  \n",
       "3              29  \n",
       "4              16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 訓練與測試資料集\n",
    "train = pd.read_csv('../../raw_data/intern_homework_train_dataset.csv')\n",
    "test = pd.read_csv('../../raw_data/intern_homework_public_test_dataset.csv')\n",
    "\n",
    "# 預測資料集\n",
    "private_test = pd.read_csv('../../raw_data/intern_homework_private_test_dataset.csv')\n",
    "\n",
    "# 觀看前 5 筆資料\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "觀察 Feature 種類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like_count_1h</th>\n",
       "      <th>like_count_2h</th>\n",
       "      <th>like_count_3h</th>\n",
       "      <th>like_count_4h</th>\n",
       "      <th>like_count_5h</th>\n",
       "      <th>like_count_6h</th>\n",
       "      <th>comment_count_1h</th>\n",
       "      <th>comment_count_2h</th>\n",
       "      <th>comment_count_3h</th>\n",
       "      <th>comment_count_4h</th>\n",
       "      <th>comment_count_5h</th>\n",
       "      <th>comment_count_6h</th>\n",
       "      <th>forum_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>forum_stats</th>\n",
       "      <th>like_count_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.097500</td>\n",
       "      <td>5.224340</td>\n",
       "      <td>7.67098</td>\n",
       "      <td>10.249960</td>\n",
       "      <td>12.733960</td>\n",
       "      <td>15.048340</td>\n",
       "      <td>4.463440</td>\n",
       "      <td>6.798680</td>\n",
       "      <td>9.004460</td>\n",
       "      <td>11.088480</td>\n",
       "      <td>12.993100</td>\n",
       "      <td>14.726880</td>\n",
       "      <td>477643.067400</td>\n",
       "      <td>498339.91460</td>\n",
       "      <td>116.125080</td>\n",
       "      <td>45.194840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.274621</td>\n",
       "      <td>24.516945</td>\n",
       "      <td>29.24685</td>\n",
       "      <td>35.367611</td>\n",
       "      <td>43.009398</td>\n",
       "      <td>51.984946</td>\n",
       "      <td>51.510914</td>\n",
       "      <td>60.982037</td>\n",
       "      <td>69.300161</td>\n",
       "      <td>77.946028</td>\n",
       "      <td>87.371928</td>\n",
       "      <td>99.403843</td>\n",
       "      <td>286979.254083</td>\n",
       "      <td>289767.40654</td>\n",
       "      <td>206.109233</td>\n",
       "      <td>180.888108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>174372.000000</td>\n",
       "      <td>243660.75000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>497426.000000</td>\n",
       "      <td>501998.00000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>717594.000000</td>\n",
       "      <td>751721.50000</td>\n",
       "      <td>93.700000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4044.000000</td>\n",
       "      <td>4797.000000</td>\n",
       "      <td>5098.00000</td>\n",
       "      <td>5362.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>6588.000000</td>\n",
       "      <td>4902.000000</td>\n",
       "      <td>4923.000000</td>\n",
       "      <td>4928.000000</td>\n",
       "      <td>4928.000000</td>\n",
       "      <td>5969.000000</td>\n",
       "      <td>8031.000000</td>\n",
       "      <td>998778.000000</td>\n",
       "      <td>999998.00000</td>\n",
       "      <td>1128.300000</td>\n",
       "      <td>13297.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       like_count_1h  like_count_2h  like_count_3h  like_count_4h  \\\n",
       "count   50000.000000   50000.000000    50000.00000   50000.000000   \n",
       "mean        3.097500       5.224340        7.67098      10.249960   \n",
       "std        19.274621      24.516945       29.24685      35.367611   \n",
       "min         0.000000       0.000000        0.00000       0.000000   \n",
       "25%         0.000000       1.000000        2.00000       2.000000   \n",
       "50%         1.000000       3.000000        4.00000       5.000000   \n",
       "75%         3.000000       5.000000        7.00000       9.000000   \n",
       "max      4044.000000    4797.000000     5098.00000    5362.000000   \n",
       "\n",
       "       like_count_5h  like_count_6h  comment_count_1h  comment_count_2h  \\\n",
       "count   50000.000000   50000.000000      50000.000000      50000.000000   \n",
       "mean       12.733960      15.048340          4.463440          6.798680   \n",
       "std        43.009398      51.984946         51.510914         60.982037   \n",
       "min         0.000000       0.000000          0.000000          0.000000   \n",
       "25%         3.000000       3.000000          0.000000          1.000000   \n",
       "50%         5.000000       6.000000          1.000000          3.000000   \n",
       "75%        11.000000      12.000000          4.000000          6.000000   \n",
       "max      5822.000000    6588.000000       4902.000000       4923.000000   \n",
       "\n",
       "       comment_count_3h  comment_count_4h  comment_count_5h  comment_count_6h  \\\n",
       "count      50000.000000      50000.000000      50000.000000      50000.000000   \n",
       "mean           9.004460         11.088480         12.993100         14.726880   \n",
       "std           69.300161         77.946028         87.371928         99.403843   \n",
       "min            0.000000          0.000000          0.000000          0.000000   \n",
       "25%            1.000000          1.000000          2.000000          2.000000   \n",
       "50%            4.000000          5.000000          5.000000          6.000000   \n",
       "75%            9.000000         11.000000         13.000000         14.000000   \n",
       "max         4928.000000       4928.000000       5969.000000       8031.000000   \n",
       "\n",
       "            forum_id     author_id   forum_stats  like_count_24h  \n",
       "count   50000.000000   50000.00000  50000.000000    50000.000000  \n",
       "mean   477643.067400  498339.91460    116.125080       45.194840  \n",
       "std    286979.254083  289767.40654    206.109233      180.888108  \n",
       "min       321.000000       2.00000      0.000000        5.000000  \n",
       "25%    174372.000000  243660.75000     16.300000        7.000000  \n",
       "50%    497426.000000  501998.00000     38.000000       13.000000  \n",
       "75%    717594.000000  751721.50000     93.700000       30.000000  \n",
       "max    998778.000000  999998.00000   1128.300000    13297.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2623/3188520339.py:1: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train.describe(include=np.object)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49158</td>\n",
       "      <td>49654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>.</td>\n",
       "      <td>2022-10-05 05:30:44 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>72</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title               created_at\n",
       "count   50000                    50000\n",
       "unique  49158                    49654\n",
       "top         .  2022-10-05 05:30:44 UTC\n",
       "freq       72                        8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns 有 Index(['title', 'created_at', 'like_count_1h', 'like_count_2h',\n",
      "       'like_count_3h', 'like_count_4h', 'like_count_5h', 'like_count_6h',\n",
      "       'comment_count_1h', 'comment_count_2h', 'comment_count_3h',\n",
      "       'comment_count_4h', 'comment_count_5h', 'comment_count_6h', 'forum_id',\n",
      "       'author_id', 'forum_stats', 'like_count_24h'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Feature Columns 有',train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "屬於 文字 的 Feature 有 ['title']\n",
      "屬於 數字 的 Feature 有 ['created_at', 'like_count_1h', 'like_count_2h', 'like_count_3h', 'like_count_4h', 'like_count_5h', 'like_count_6h', 'comment_count_1h', 'comment_count_2h', 'comment_count_3h', 'comment_count_4h', 'comment_count_5h', 'comment_count_6h', 'forum_stats']\n",
      "屬於 ID / 布林 / 類別 的 Feature 有 ['author_id', 'forum_id']\n"
     ]
    }
   ],
   "source": [
    "text_feature = ['title']\n",
    "number_feature = ['created_at', 'like_count_1h', 'like_count_2h','like_count_3h', 'like_count_4h', 'like_count_5h', 'like_count_6h',\n",
    "       'comment_count_1h', 'comment_count_2h', 'comment_count_3h','comment_count_4h', 'comment_count_5h', 'comment_count_6h','forum_stats']\n",
    "category_feature = ['author_id','forum_id']\n",
    "print('屬於 文字 的 Feature 有',text_feature)\n",
    "print('屬於 數字 的 Feature 有',number_feature)\n",
    "print('屬於 ID / 布林 / 類別 的 Feature 有',category_feature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選 Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省略掉 作者 ID  因為我認為 BART 沒辦法了解這些 Feature, 會讓模型預測文字輸出帶來 noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要刪除的 column names，並使用 drop 函數將這些 column 刪除\n",
    "\n",
    "drop_columns = ['author_id']\n",
    "\n",
    "train_input = train.drop(drop_columns, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfromations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理 created_by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章發佈時間 拆成 星期幾 與 小時 的函數\n",
    "def split_date(df, date_column):\n",
    "\n",
    "    # 將 created_by 欄位轉換成日期格式\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True)\n",
    "    \n",
    "    # 新增 星期幾 和 小時 欄位\n",
    "    df['weekday'] = df[date_column].dt.weekday\n",
    "    df['hour'] = df[date_column].dt.hour\n",
    "\n",
    "    # 移除 created_by 欄位\n",
    "    df = df.drop(date_column, axis=1)\n",
    "\n",
    "    # 回傳處理過的資料集\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>like_count_1h</th>\n",
       "      <th>like_count_2h</th>\n",
       "      <th>like_count_3h</th>\n",
       "      <th>like_count_4h</th>\n",
       "      <th>like_count_5h</th>\n",
       "      <th>like_count_6h</th>\n",
       "      <th>comment_count_1h</th>\n",
       "      <th>comment_count_2h</th>\n",
       "      <th>comment_count_3h</th>\n",
       "      <th>comment_count_4h</th>\n",
       "      <th>comment_count_5h</th>\n",
       "      <th>comment_count_6h</th>\n",
       "      <th>forum_id</th>\n",
       "      <th>forum_stats</th>\n",
       "      <th>like_count_24h</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我的排骨湯</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>598518</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#請益 婚禮穿搭</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>399302</td>\n",
       "      <td>63.9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>無謂的啦啦隊</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>650776</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>文學理論 課本</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>63</td>\n",
       "      <td>471023</td>\n",
       "      <td>7.9</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>一般課程</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>230184</td>\n",
       "      <td>36.2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title  like_count_1h  like_count_2h  like_count_3h  like_count_4h  \\\n",
       "0     我的排骨湯             12             15             15             15   \n",
       "1  #請益 婚禮穿搭              0              0              3              4   \n",
       "2    無謂的啦啦隊              3              7              8             11   \n",
       "3   文學理論 課本              2              7             11             24   \n",
       "4      一般課程              3              7              7             10   \n",
       "\n",
       "   like_count_5h  like_count_6h  comment_count_1h  comment_count_2h  \\\n",
       "0             16             18                10                10   \n",
       "1              4              4                 2                 5   \n",
       "2             12             14                 1                 1   \n",
       "3             26             26                 2                 2   \n",
       "4             10             11                15                26   \n",
       "\n",
       "   comment_count_3h  comment_count_4h  comment_count_5h  comment_count_6h  \\\n",
       "0                10                10                10                10   \n",
       "1                 8                 9                 9                 9   \n",
       "2                 2                 3                 3                 3   \n",
       "3                 8                32                38                63   \n",
       "4                35                38                48                49   \n",
       "\n",
       "   forum_id  forum_stats  like_count_24h  weekday  hour  \n",
       "0    598518          0.7              26        2    14  \n",
       "1    399302         63.9              11        2    14  \n",
       "2    650776         19.2              19        3     7  \n",
       "3    471023          7.9              29        1    11  \n",
       "4    230184         36.2              16        0    10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = split_date(train_input, 'created_at')\n",
    "\n",
    "# 顯示處理過的資料集\n",
    "train_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'like_count_1h', 'like_count_2h', 'like_count_3h',\n",
       "       'like_count_4h', 'like_count_5h', 'like_count_6h', 'comment_count_1h',\n",
       "       'comment_count_2h', 'comment_count_3h', 'comment_count_4h',\n",
       "       'comment_count_5h', 'comment_count_6h', 'forum_id', 'forum_stats',\n",
       "       'like_count_24h', 'weekday', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存資料至指定格式(csv檔)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples train-val-test\n",
      "40000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = np.split(train_input.sample(frac=1), [int(.8*len(train_input)), int(.9 * len(train_input))])\n",
    "print('Num examples train-val-test')\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "train_df.to_csv('train.csv')\n",
    "val_df.to_csv('val.csv')\n",
    "test_df.to_csv('test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定義要處理的資料格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forum_id forum_stats\n",
    "text_cols = ['title']\n",
    "cat_cols = []\n",
    "numerical_cols = ['weekday', 'hour', 'like_count_1h', 'like_count_2h', 'like_count_3h',\n",
    "       'like_count_4h', 'like_count_5h', 'like_count_6h', 'comment_count_1h',\n",
    "       'comment_count_2h', 'comment_count_3h', 'comment_count_4h',\n",
    "       'comment_count_5h', 'comment_count_6h','forum_stats']\n",
    "\n",
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'num_cols': numerical_cols,\n",
    "    'cat_cols': cat_cols,\n",
    "    'label_col': 'like_count_24h'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "  \"\"\"\n",
    "  Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "  \"\"\"\n",
    "\n",
    "  model_name_or_path: str = field(\n",
    "      metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "  )\n",
    "  config_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "  )\n",
    "  tokenizer_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "  )\n",
    "  cache_dir: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "  )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "  \"\"\"\n",
    "  Arguments pertaining to how we combine tabular features\n",
    "  Using `HfArgumentParser` we can turn this class\n",
    "  into argparse arguments to be able to specify them on\n",
    "  the command line.\n",
    "  \"\"\"\n",
    "\n",
    "  data_path: str = field(metadata={\n",
    "                            'help': 'the path to the csv file containing the dataset'\n",
    "                        })\n",
    "  column_info_path: str = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n",
    "  })\n",
    "\n",
    "  column_info: dict = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n",
    "                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n",
    "  })\n",
    "\n",
    "  categorical_encode_type: str = field(default='none',\n",
    "                                        metadata={\n",
    "                                            'help': 'sklearn encoder to use for categorical data',\n",
    "                                            'choices': ['ohe', 'binary', 'label', 'none']\n",
    "                                        })\n",
    "  numerical_transformer_method: str = field(default='none',\n",
    "                                            metadata={\n",
    "                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n",
    "                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n",
    "                                            })\n",
    "  task: str = field(default=\"classification\",\n",
    "                    metadata={\n",
    "                        \"help\": \"The downstream training task\",\n",
    "                        \"choices\": [\"classification\", \"regression\"]\n",
    "                    })\n",
    "\n",
    "  mlp_division: int = field(default=4,\n",
    "                            metadata={\n",
    "                                'help': 'the ratio of the number of '\n",
    "                                        'hidden dims in a current layer to the next MLP layer'\n",
    "                            })\n",
    "  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n",
    "                                    metadata={\n",
    "                                        'help': 'method to combine categorical and numerical features, '\n",
    "                                                'see README for all the method'\n",
    "                                    })\n",
    "  mlp_dropout: float = field(default=0.1,\n",
    "                              metadata={\n",
    "                                'help': 'dropout ratio used for MLP layers'\n",
    "                              })\n",
    "  numerical_bn: bool = field(default=True,\n",
    "                              metadata={\n",
    "                                  'help': 'whether to use batchnorm on numerical features'\n",
    "                              })\n",
    "  use_simple_classifier: str = field(default=True,\n",
    "                                      metadata={\n",
    "                                          'help': 'whether to use single layer or MLP as final classifier'\n",
    "                                      })\n",
    "  mlp_act: str = field(default='relu',\n",
    "                        metadata={\n",
    "                            'help': 'the activation function to use for finetuning layers',\n",
    "                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n",
    "                        })\n",
    "  gating_beta: float = field(default=0.2,\n",
    "                              metadata={\n",
    "                                  'help': \"the beta hyperparameters used for gating tabular data \"\n",
    "                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n",
    "                              })\n",
    "\n",
    "  def __post_init__(self):\n",
    "      assert self.column_info != self.column_info_path\n",
    "      if self.column_info is None and self.column_info_path:\n",
    "          with open(self.column_info_path, 'r') as f:\n",
    "              self.column_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 11:04:22.875750: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 11:04:22.978098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-08 11:04:23.436510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-08 11:04:23.436630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-08 11:04:23.436638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path='bert-base-chinese'\n",
    ")\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path='.',\n",
    "    combine_feat_method='concat',\n",
    "    column_info=column_info_dict,\n",
    "    task='regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs/model_name\",\n",
    "    logging_dir=\"./logs/runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"MAPE\",\n",
    "    greater_is_better = False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=25,\n",
    "    eval_steps=250,\n",
    "    eval_accumulation_steps = 1,\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified tokenizer:  bert-base-chinese\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "print('Specified tokenizer: ', tokenizer_path_or_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path_or_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset csv to torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "    data_args.data_path,\n",
    "    data_args.column_info['text_cols'],\n",
    "    tokenizer,\n",
    "    categorical_encode_type='none',\n",
    "    numerical_transformer_method='quantile_normal',\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    # categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols=data_args.column_info['num_cols'],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "tabular_config = TabularConfig(num_labels=1,\n",
    "                            #    cat_feat_dim=train_dataset.cat_feats.shape[1],\n",
    "                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n",
    "                               **vars(data_args))\n",
    "config.tabular_config = tabular_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertWithTabular: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertWithTabular were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['tabular_combiner.num_bn.running_var', 'tabular_combiner.num_bn.num_batches_tracked', 'classifier.weight', 'tabular_combiner.num_bn.weight', 'tabular_combiner.num_bn.running_mean', 'tabular_classifier.bias', 'tabular_combiner.num_bn.bias', 'classifier.bias', 'tabular_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithTabular.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "\n",
    "def regression_metrics(p: EvalPrediction):\n",
    "\n",
    "    \n",
    "    pred_labels = np.squeeze(p.predictions[0])\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(labels,pred_labels)\n",
    "    mae = mean_absolute_error(labels,pred_labels)\n",
    "    result = {\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=regression_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1875\n",
      "  Number of trainable parameters = 102270000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhankystyle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/Dcard/model/transformers_with_tabular_data/wandb/run-20230408_110530-y733ql0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hankystyle/huggingface/runs/y733ql0r' target=\"_blank\">swift-puddle-57</a></strong> to <a href='https://wandb.ai/hankystyle/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hankystyle/huggingface' target=\"_blank\">https://wandb.ai/hankystyle/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hankystyle/huggingface/runs/y733ql0r' target=\"_blank\">https://wandb.ai/hankystyle/huggingface/runs/y733ql0r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 626/1875 04:40 < 09:20, 2.23 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./logs/model_name/checkpoint-625\n",
      "Configuration saved in ./logs/model_name/checkpoint-625/config.json\n",
      "Model weights saved in ./logs/model_name/checkpoint-625/pytorch_model.bin\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./logs/model_name/checkpoint-1250\n",
      "Configuration saved in ./logs/model_name/checkpoint-1250/config.json\n",
      "Model weights saved in ./logs/model_name/checkpoint-1250/pytorch_model.bin\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./logs/model_name/checkpoint-1875\n",
      "Configuration saved in ./logs/model_name/checkpoint-1875/config.json\n",
      "Model weights saved in ./logs/model_name/checkpoint-1875/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./logs/model_name/checkpoint-625 (score: 1.6138845862437712).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=37095.42175, metrics={'train_runtime': 971.6774, 'train_samples_per_second': 123.498, 'train_steps_per_second': 1.93, 'total_flos': 1.3875128928e+16, 'train_loss': 37095.42175, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "  \"\"\"\n",
    "  Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "  \"\"\"\n",
    "\n",
    "  model_name_or_path: str = field(\n",
    "      metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "  )\n",
    "  config_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "  )\n",
    "  tokenizer_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "  )\n",
    "  cache_dir: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "  )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "  \"\"\"\n",
    "  Arguments pertaining to how we combine tabular features\n",
    "  Using `HfArgumentParser` we can turn this class\n",
    "  into argparse arguments to be able to specify them on\n",
    "  the command line.\n",
    "  \"\"\"\n",
    "\n",
    "  data_path: str = field(metadata={\n",
    "                            'help': 'the path to the csv file containing the dataset'\n",
    "                        })\n",
    "  column_info_path: str = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n",
    "  })\n",
    "\n",
    "  column_info: dict = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n",
    "                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n",
    "  })\n",
    "\n",
    "  categorical_encode_type: str = field(default='none',\n",
    "                                        metadata={\n",
    "                                            'help': 'sklearn encoder to use for categorical data',\n",
    "                                            'choices': ['ohe', 'binary', 'label', 'none']\n",
    "                                        })\n",
    "  numerical_transformer_method: str = field(default='none',\n",
    "                                            metadata={\n",
    "                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n",
    "                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n",
    "                                            })\n",
    "  task: str = field(default=\"classification\",\n",
    "                    metadata={\n",
    "                        \"help\": \"The downstream training task\",\n",
    "                        \"choices\": [\"classification\", \"regression\"]\n",
    "                    })\n",
    "\n",
    "  mlp_division: int = field(default=4,\n",
    "                            metadata={\n",
    "                                'help': 'the ratio of the number of '\n",
    "                                        'hidden dims in a current layer to the next MLP layer'\n",
    "                            })\n",
    "  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n",
    "                                    metadata={\n",
    "                                        'help': 'method to combine categorical and numerical features, '\n",
    "                                                'see README for all the method'\n",
    "                                    })\n",
    "  mlp_dropout: float = field(default=0.1,\n",
    "                              metadata={\n",
    "                                'help': 'dropout ratio used for MLP layers'\n",
    "                              })\n",
    "  numerical_bn: bool = field(default=True,\n",
    "                              metadata={\n",
    "                                  'help': 'whether to use batchnorm on numerical features'\n",
    "                              })\n",
    "  use_simple_classifier: str = field(default=True,\n",
    "                                      metadata={\n",
    "                                          'help': 'whether to use single layer or MLP as final classifier'\n",
    "                                      })\n",
    "  mlp_act: str = field(default='relu',\n",
    "                        metadata={\n",
    "                            'help': 'the activation function to use for finetuning layers',\n",
    "                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n",
    "                        })\n",
    "  gating_beta: float = field(default=0.2,\n",
    "                              metadata={\n",
    "                                  'help': \"the beta hyperparameters used for gating tabular data \"\n",
    "                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n",
    "                              })\n",
    "\n",
    "  def __post_init__(self):\n",
    "      assert self.column_info != self.column_info_path\n",
    "      if self.column_info is None and self.column_info_path:\n",
    "          with open(self.column_info_path, 'r') as f:\n",
    "              self.column_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forum_id forum_stats\n",
    "text_cols = ['title']\n",
    "cat_cols = ['forum_id']\n",
    "numerical_cols = ['weekday', 'hour', 'like_count_1h', 'like_count_2h', 'like_count_3h',\n",
    "       'like_count_4h', 'like_count_5h', 'like_count_6h', 'comment_count_1h',\n",
    "       'comment_count_2h', 'comment_count_3h', 'comment_count_4h',\n",
    "       'comment_count_5h', 'comment_count_6h','forum_stats']\n",
    "\n",
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'num_cols': numerical_cols,\n",
    "    'cat_cols': cat_cols,\n",
    "    'label_col': 'like_count_24h'\n",
    "}\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    config_name = 'logs/model_name/checkpoint-1875',\n",
    "    model_name_or_path='bert-base-chinese'\n",
    ")\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path='.',\n",
    "    combine_feat_method='concat',\n",
    "    column_info=column_info_dict,\n",
    "    task='regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelWithTabular.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "print('Specified tokenizer: ', tokenizer_path_or_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path_or_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "    data_args.data_path,\n",
    "    data_args.column_info['text_cols'],\n",
    "    tokenizer,\n",
    "    categorical_encode_type='none',\n",
    "    numerical_transformer_method='none',\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols=data_args.column_info['num_cols'],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "tabular_config = TabularConfig(num_labels=1,\n",
    "                               cat_feat_dim=train_dataset.cat_feats.shape[1],\n",
    "                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n",
    "                               **vars(data_args))\n",
    "config.tabular_config = tabular_config\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs/model_name\",\n",
    "    logging_dir=\"./logs/runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"MAPE\",\n",
    "    greater_is_better = False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=25,\n",
    "    eval_steps=250,\n",
    "    eval_accumulation_steps = 1,\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "\n",
    "def regression_metrics(p: EvalPrediction):\n",
    "\n",
    "    \n",
    "    pred_labels = np.squeeze(p.predictions[0])\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(labels,pred_labels)\n",
    "    mae = mean_absolute_error(labels,pred_labels)\n",
    "    result = {\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=regression_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.squeeze(predictions[0])\n",
    "labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
